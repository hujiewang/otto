
Model #3
=========================================================================

optim_func=optim.sgd
optim_params = {
  learningRate = 1e-1,
  learningRateDecay = 1e-6,
  weightDecay = 1e-5,
  dampening = 0.5,
  momentum = 0.95,
  --nesterov,
}
opt={
  createData = false,
  epochs = 100000,
  batch_size = 200000,
  predict = false,
  save_gap = 50,
  cuda=true,
  --model_file = 'model.dat'
}
model = nn.Sequential()

model:add(nn.Linear(93,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,256))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(256))
model:add(nn.Dropout())

model:add(nn.Linear(256,9))

model:add(nn.LogSoftMax())

Model #4
=========================================================================
loss = 0.49530966832728

optim_func=optim.rmsprop
optim_params = {
  learningRate = 1e-2,
  learningRateDecay = 1e-6,
  weightDecay = 1e-5,
  dampening = 0.5,
  momentum = 0.95,
  --nesterov,
}
opt={
  createData = false,
  epochs = 100000,
  batch_size = 8192,
  predict = false,
  save_gap = 50,
  cuda=true,
  plot=false,
  sparse_init = true,
  standardize = true,
  --model_file = 'model.dat'
}



model = nn.Sequential()

--model:add(nn.Dropout())

model:add(nn.Linear(93,512))
model:add(nn.ReLU(512))
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU(512))
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU(512))
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,9))

model:add(nn.LogSoftMax())

Model #5
=========================================================================
valid loss: 0.4482353925705

optim_func=optim.rmsprop
optim_params = {
  learningRate = 1e-2,
  learningRateDecay = 1e-6,
  weightDecay = 1e-4,
  dampening = 0.5,
  momentum = 0.95,
  --nesterov,
}
opt={
  createData = false,
  epochs = 100000,
  batch_size = 10000,
  predict = false,
  save_gap = 10,
  cuda=true,
  plot=false,
  sparse_init = true,
  standardize = true,
  --model_file = 'model.dat'
}


model = nn.Sequential()

model:add(nn.Dropout(0.2))

model:add(nn.Linear(93,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())


model:add(nn.Linear(512,9))

model:add(nn.LogSoftMax())

Model #6
=========================================================================
valid loss: 0.44306018948555	

optim_func=optim.rmsprop
optim_params = {
  learningRate = 1e-2,
  learningRateDecay = 1e-6,
  weightDecay = 1e-4,
  dampening = 0.5,
  momentum = 0.95,
  --nesterov,
}
opt={
  createData = false,
  epochs = 100000,
  batch_size = 10000,
  predict = false,
  save_gap = 10,
  cuda=true,
  plot=false,
  sparse_init = true,
  standardize = true,
  --model_file = 'model.dat'
}

models={}

model = nn.Sequential()

model:add(nn.BatchNormalization(93))
model:add(nn.Dropout(0.2))

model:add(nn.Linear(93,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,9))

model:add(nn.LogSoftMax())

Model #7
=========================================================================
valid loss: 0.4416321516037

optim_func=optim.rmsprop
optim_params = {
  learningRate = 1e-2,
  learningRateDecay = 1e-6,
  weightDecay = 1e-4,
  dampening = 0.5,
  momentum = 0.95,
  --nesterov,
}
opt={
  createData = false,
  epochs = 100000,
  batch_size = 10000,
  predict = false,
  save_gap = 10,
  cuda=true,
  plot=false,
  sparse_init = true,
  standardize = true,
  --model_file = 'model.dat'
}

local n = 3

experts = nn.ConcatTable()
for i = 1, n do
  local expert = nn.Sequential()
  expert:add(nn.BatchNormalization(93))
  expert:add(nn.Dropout(0.25))

  expert:add(nn.Linear(93,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,9))

  expert:add(nn.LogSoftMax())
  experts:add(expert)
end

gater = nn.Sequential()

gater:add(nn.Linear(93,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,n))

gater:add(nn.SoftMax())

trunk = nn.ConcatTable()
trunk:add(gater)
trunk:add(experts)

model = nn.Sequential()
model:add(trunk)
model:add(nn.MixtureTable())

Model #8
=========================================================================
valid loss: 0.44244325884851	

local n = 3

experts = nn.ConcatTable()
for i = 1, n do
  local expert = nn.Sequential()
  expert:add(nn.BatchNormalization(93))
  expert:add(nn.Dropout(0.22))

  expert:add(nn.Linear(93,512))
  expert:add(nn.PReLU(512))
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.PReLU(512))
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.PReLU(512))
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,9))

  expert:add(nn.LogSoftMax())
  experts:add(expert)
end

gater = nn.Sequential()

gater:add(nn.Linear(93,512))
gater:add(nn.PReLU(512))

gater:add(nn.Linear(512,512))
gater:add(nn.PReLU(512))

gater:add(nn.Linear(512,512))
gater:add(nn.PReLU(512))

gater:add(nn.Linear(512,n))

gater:add(nn.SoftMax())

trunk = nn.ConcatTable()
trunk:add(gater)
trunk:add(experts)

model = nn.Sequential()
model:add(trunk)
model:add(nn.MixtureTable())

Model #9
=========================================================================
valid loss: 0.43499

average of model #8 and model #7

Model #10
=========================================================================
valid loss: 0.42800

average of model #6 #7 #8

Model #11
=========================================================================
valid loss: 0.44462525844574	

local n = 3

experts = nn.ConcatTable()
for i = 1, n do
  local expert = nn.Sequential()
  expert:add(nn.BatchNormalization(93))
  expert:add(nn.Dropout(0.27))

  expert:add(nn.Linear(93,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,512))
  expert:add(nn.ReLU())
  expert:add(nn.BatchNormalization(512))
  expert:add(nn.Dropout())

  expert:add(nn.Linear(512,9))

  expert:add(nn.LogSoftMax())
  experts:add(expert)
end

gater = nn.Sequential()

gater:add(nn.Linear(93,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,512))
gater:add(nn.ReLU())

gater:add(nn.Linear(512,n))

gater:add(nn.SoftMax())

trunk = nn.ConcatTable()
trunk:add(gater)
trunk:add(experts)

model = nn.Sequential()
model:add(trunk)
model:add(nn.MixtureTable())

Model #12
=========================================================================
valid loss: 0.44414782524109	

models={}

model = nn.Sequential()

model:add(nn.BatchNormalization(93))
model:add(nn.Dropout(0.25))

model:add(nn.Linear(93,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,512))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(512))
model:add(nn.Dropout())

model:add(nn.Linear(512,9))

model:add(nn.LogSoftMax())

Model #13
=========================================================================
valid loss: 0.44397607445717

models={}

model = nn.Sequential()

model:add(nn.BatchNormalization(93))
model:add(nn.Dropout(0.25))

model:add(nn.Linear(93,800))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(800))
model:add(nn.Dropout())

model:add(nn.Linear(800,800))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(800))
model:add(nn.Dropout())

model:add(nn.Linear(800,800))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(800))
model:add(nn.Dropout())

model:add(nn.Linear(800,9))

model:add(nn.LogSoftMax())

Model #14
=========================================================================
valid loss: 0.44450652599335	

models={}

model = nn.Sequential()

model:add(nn.BatchNormalization(93))
model:add(nn.Dropout(0.25))

model:add(nn.Linear(93,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,9))

model:add(nn.LogSoftMax())

Model #15
=========================================================================
valid loss: 	0.44331285357475	

models={}

model = nn.Sequential()

model:add(nn.BatchNormalization(93))
model:add(nn.Dropout(0.30))

model:add(nn.Linear(93,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,1024))
model:add(nn.ReLU())
model:add(nn.BatchNormalization(1024))
model:add(nn.Dropout())

model:add(nn.Linear(1024,9))

model:add(nn.LogSoftMax())